{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste Semantix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Qual o objetivo do comando cache em Spark?\n",
    "\n",
    "Resposta: Manter o conjunto de dados em memória em todo o cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em\n",
    "MapReduce. Por quê?\n",
    "\n",
    "Resposta: Enquanto o Hadoop move os dados por meio dos discos e da rede, o Spark move os dados em memória, diminuindo a quantidade I/O consideravelmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Qual é a função do SparkContext?\n",
    "\n",
    "Resposta: O SparkContext realiza a conexão com o cluster e pode ser utilizado para criar RDD's, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explique com suas palavras o que é Resilient Distributed Datasets (RDD).\n",
    "\n",
    "Resposta: RDD é um conjunto tolerante a falhas de objetos que podem ser processados em paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?\n",
    "\n",
    "Resposta: Porque o reduceByKey realiza a operação dentro das partiçoões antes de enviar para o worker responsável por realizar o reduce. Enquanto o GroupByKey não realiza a operação dentro das pertições, gerando maior tráfego de dados para o worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explique o que o código Scala abaixo faz.\n",
    "'''scala\n",
    "val textFile = sc.textFile(\"hdfs://...\")\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    ".map(word => (word, 1))\n",
    ".reduceByKey(_ + _)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "'''\n",
    "\n",
    "Resposta: O código realiza uma contagem da quantidade de ocorrência de cada palavra.\n",
    "* flatMap -> obtém o cada palavra que ocorreu na linha\n",
    "* map -> para cada palavra, atribui o valor 1 referente a uma ocorrência\n",
    "* reduceByKey -> realiza a soma de cada palavra que ocorreu mais de uma vez\n",
    "E salva em um arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função cria as colunas no dataframe conforme descritas no PDF\n",
    "# com seus respectivos valores por meio de expressão regular\n",
    "def build_df(df_source):\n",
    "    df_clean = df_source.withColumn(\n",
    "        'host',\n",
    "        regexp_replace('value', '^([^\\s]+)\\s+.*', r'$1')\n",
    "    )\n",
    "    df_clean.cache()\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'timestamp',\n",
    "        regexp_replace('value', '^[^\\[]+\\[([^\\]]+)\\].*', r'$1')\n",
    "    )\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'date',\n",
    "        regexp_replace('value', '^[^\\[]+\\[([^:]+)\\:.*', r'$1')\n",
    "    )\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'http_request',\n",
    "        regexp_replace('value', '^[^\\\"]+\\\"([^\\\"]+)\\\".*', r'$1')\n",
    "    )\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'url',\n",
    "        regexp_replace('value', '^[^\\\"]+\\\"\\w+\\s+([^\\s]+)\\s+.*', r'$1')\n",
    "    )\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'request_status',\n",
    "        regexp_replace('value', '.*\\s+(\\d+)\\s[0-9\\-]+$', r'$1').cast('Integer')\n",
    "    )\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'total_bytes',\n",
    "        regexp_replace('value', '.*\\s(\\d+)$', r'$1').cast('Long')\n",
    "    )\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o SparkSession para utilizarmos o conceito de dataframe do Spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Teste Semantix\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os dataframes a partir dos arquivos\n",
    "df_source_july = spark.read.text('NASA_access_log_Jul95')\n",
    "df_source_aug = spark.read.text('NASA_access_log_Aug95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os dataframes com todas as colunas descritas no PDF\n",
    "df_july = build_df(df_source_july)\n",
    "df_aug = build_df(df_source_aug)\n",
    "df_all = df_july.union(df_aug)\n",
    "df_all.cache()\n",
    "df_all = df_all.filter(df_all['request_status'].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Número de hosts únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137978"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.select('host').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. O total de erros 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20901\n"
     ]
    }
   ],
   "source": [
    "df_404 = df_all.filter(df_all['request_status'] == 404).select(['date', 'url', 'request_status'])\n",
    "df_404.cache()\n",
    "print(df_404.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Os 5 URLs que mais causaram erro 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-----+\n",
      "|url                                         |count|\n",
      "+--------------------------------------------+-----+\n",
      "|/pub/winvn/readme.txt                       |2004 |\n",
      "|/pub/winvn/release.txt                      |1732 |\n",
      "|/shuttle/missions/STS-69/mission-STS-69.html|682  |\n",
      "|/shuttle/missions/sts-68/ksc-upclose.gif    |426  |\n",
      "|/history/apollo/a-001/a-001-patch-small.gif |384  |\n",
      "+--------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_404.groupBy('url').count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Quantidade de erros 404 por dia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       date|sum(1)|\n",
      "+-----------+------+\n",
      "|02/Jul/1995|   291|\n",
      "|21/Aug/1995|   305|\n",
      "|06/Aug/1995|   373|\n",
      "|16/Jul/1995|   257|\n",
      "|07/Aug/1995|   537|\n",
      "|11/Aug/1995|   263|\n",
      "|27/Jul/1995|   336|\n",
      "|07/Jul/1995|   570|\n",
      "|17/Jul/1995|   406|\n",
      "|15/Jul/1995|   254|\n",
      "|18/Jul/1995|   465|\n",
      "|26/Jul/1995|   336|\n",
      "|03/Aug/1995|   304|\n",
      "|18/Aug/1995|   256|\n",
      "|17/Aug/1995|   271|\n",
      "|14/Aug/1995|   287|\n",
      "|10/Jul/1995|   398|\n",
      "|04/Jul/1995|   359|\n",
      "|20/Aug/1995|   312|\n",
      "|20/Jul/1995|   428|\n",
      "|24/Aug/1995|   420|\n",
      "|27/Aug/1995|   370|\n",
      "|13/Aug/1995|   216|\n",
      "|15/Aug/1995|   327|\n",
      "|28/Jul/1995|    94|\n",
      "|12/Jul/1995|   471|\n",
      "|25/Aug/1995|   415|\n",
      "|06/Jul/1995|   640|\n",
      "|22/Aug/1995|   288|\n",
      "|08/Aug/1995|   391|\n",
      "|22/Jul/1995|   192|\n",
      "|01/Jul/1995|   316|\n",
      "|23/Jul/1995|   233|\n",
      "|21/Jul/1995|   334|\n",
      "|23/Aug/1995|   345|\n",
      "|19/Aug/1995|   209|\n",
      "|24/Jul/1995|   328|\n",
      "|26/Aug/1995|   366|\n",
      "|31/Aug/1995|   526|\n",
      "|28/Aug/1995|   410|\n",
      "|04/Aug/1995|   346|\n",
      "|12/Aug/1995|   196|\n",
      "|19/Jul/1995|   639|\n",
      "|30/Aug/1995|   571|\n",
      "|05/Aug/1995|   236|\n",
      "|08/Jul/1995|   302|\n",
      "|09/Jul/1995|   348|\n",
      "|01/Aug/1995|   243|\n",
      "|25/Jul/1995|   461|\n",
      "|05/Jul/1995|   497|\n",
      "|11/Jul/1995|   471|\n",
      "|29/Aug/1995|   420|\n",
      "|13/Jul/1995|   532|\n",
      "|16/Aug/1995|   259|\n",
      "|09/Aug/1995|   279|\n",
      "|10/Aug/1995|   315|\n",
      "|14/Jul/1995|   413|\n",
      "|03/Jul/1995|   474|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_404.groupBy('date').agg(_sum(lit(1))).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. O total de bytes retornados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|sum(total_bytes)|\n",
      "+----------------+\n",
      "|     65524314915|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all.agg(_sum('total_bytes')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
